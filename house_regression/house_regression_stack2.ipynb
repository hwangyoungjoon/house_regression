{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.linear_model import Ridge, LassoCV,LassoLarsCV, ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats import skew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(prediction,score):\n",
    "    now = datetime.datetime.now()\n",
    "    sub_file = 'submission_'+str(score)+'_'+str(now.strftime(\"%Y-%m-%d-%H-%M\"))+'.csv'\n",
    "    #sub_file = 'prediction_training.csv'\n",
    "    print ('Creating submission: ', sub_file)\n",
    "    pd.DataFrame({'Id': test['Id'].values, 'SalePrice': prediction}).to_csv(sub_file, index=False)\n",
    "\n",
    "# train need to be test when do test prediction\n",
    "def data_preprocess(train,test):\n",
    "    outlier_idx = [4,11,13,20,46,66,70,167,178,185,199, 224,261, 309,313,318, 349,412,423,440,454,477,478, 523,540, 581,588,595,654,688, 691, 774, 798, 875, 898,926,970,987,1027,1109, 1169,1182,1239, 1256,1298,1324,1353,1359,1405,1442,1447]\n",
    "    train.drop(train.index[outlier_idx],inplace=True)\n",
    "    all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
    "                          test.loc[:,'MSSubClass':'SaleCondition']))\n",
    "    \n",
    "    to_delete = ['Alley','FireplaceQu','PoolQC','Fence','MiscFeature']\n",
    "    all_data = all_data.drop(to_delete,axis=1)\n",
    "\n",
    "    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "    #log transform skewed numeric features\n",
    "    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n",
    "    skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "    skewed_feats = skewed_feats.index\n",
    "    all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
    "    all_data = pd.get_dummies(all_data)\n",
    "    all_data = all_data.fillna(all_data.mean())\n",
    "    X_train = all_data[:train.shape[0]]\n",
    "    X_test = all_data[train.shape[0]:]\n",
    "    y = train.SalePrice\n",
    "\n",
    "    return X_train,X_test,y\n",
    "\n",
    "\n",
    "def mean_squared_error_(ground_truth, predictions):\n",
    "    return mean_squared_error(ground_truth, predictions) ** 0.5\n",
    "RMSE = make_scorer(mean_squared_error_, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ensemble(object):\n",
    "    def __init__(self, n_folds, stacker, base_models):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "    def fit_predict(self,train,test,ytr):\n",
    "        X = train.values\n",
    "\ty = ytr.values\n",
    "        T = test.values\n",
    "        folds = list(KFold(len(y), n_folds = self.n_folds, shuffle = True, random_state = 0))\n",
    "        S_train = np.zeros((X.shape[0],len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0],len(self.base_models))) \n",
    "        for i,reg in enumerate(base_models):\n",
    "            print (\"Fitting the base model...\")\n",
    "            S_test_i = np.zeros((T.shape[0],len(folds))) \n",
    "            for j, (train_idx,test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                reg.fit(X_train,y_train)\n",
    "                y_pred = reg.predict(X_holdout)[:]\n",
    "                S_train[test_idx,i] = y_pred\n",
    "                S_test_i[:,j] = reg.predict(T)[:]\n",
    "            S_test[:,i] = S_test_i.mean(1)\n",
    "         \n",
    "        print (\"Stacking base models...\")\n",
    "        # tuning the stacker\n",
    "\tparam_grid = {\n",
    "\t     'alpha': [1e-3,5e-3,1e-2,5e-2,1e-1,0.2,0.3,0.4,0.5,0.8,1e0,3,5,7,1e1],\n",
    "\t}\n",
    "\tgrid = GridSearchCV(estimator=self.stacker, param_grid=param_grid, n_jobs=1, cv=5, scoring=RMSE)\n",
    "        grid.fit(S_train, y)\n",
    "        try:\n",
    "            print('Param grid:')\n",
    "            print(param_grid)\n",
    "            print('Best Params:')\n",
    "            print(grid.best_params_)\n",
    "            print('Best CV Score:')\n",
    "            print(-grid.best_score_)\n",
    "            print('Best estimator:')\n",
    "            print(grid.best_estimator_)\n",
    "            print(message)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        y_pred = grid.predict(S_test)[:]\n",
    "        return y_pred, -grid.best_score_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\") # read train data\n",
    "test = pd.read_csv(\"./test.csv\") # read test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a model library (can be improved)\n",
    "base_models = [\n",
    "        RandomForestRegressor(\n",
    "            n_jobs=1, random_state=0,\n",
    "            n_estimators=500, max_features=14\n",
    "        ),\n",
    "        RandomForestRegressor(\n",
    "            n_jobs=1, random_state=0,\n",
    "            n_estimators=500, max_features=20,\n",
    "\t    max_depth = 7\n",
    "        ),\n",
    "        ExtraTreesRegressor(\n",
    "            n_jobs=1, random_state=0, \n",
    "            n_estimators=500, max_features=15\n",
    "        ),\n",
    "        ExtraTreesRegressor(\n",
    "            n_jobs=1, random_state=0, \n",
    "          n_estimators=500, max_features=20\n",
    "        ),\n",
    "        GradientBoostingRegressor(\n",
    "            random_state=0, \n",
    "            n_estimators=500, max_features=10, max_depth=6,\n",
    "            learning_rate=0.05, subsample=0.8\n",
    "        ),\n",
    "\tGradientBoostingRegressor(\n",
    "            random_state=0, \n",
    "            n_estimators=500, max_features=15, max_depth=6,\n",
    "            learning_rate=0.05, subsample=0.8\n",
    "        ),\n",
    "       \n",
    "\tLassoCV(alphas = [1, 0.1, 0.001, 0.0005]),\n",
    "\tKNeighborsRegressor(n_neighbors = 5),\n",
    "       \tKNeighborsRegressor(n_neighbors = 10),\n",
    "      \tKNeighborsRegressor(n_neighbors = 15),\n",
    "        KNeighborsRegressor(n_neighbors = 25),\n",
    "\tLassoLarsCV(),\n",
    "\tElasticNet(),\n",
    "\tSVR()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Fitting the base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 139 iterations, i.e. alpha=7.175e-05, with an active set of 127 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 170 iterations, alpha=4.742e-05, previous alpha=4.734e-05, with an active set of 149 regressors.\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=3.229e-04, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.358e-04, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.281e-04, with an active set of 109 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=1.276e-04, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 114 iterations, alpha=1.266e-04, previous alpha=1.256e-04, with an active set of 109 regressors.\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=1.696e-04, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=8.976e-04, with an active set of 20 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.943e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.943e-04, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.957e-04, with an active set of 79 regressors, and the smallest cholesky pivot element being 3.332e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.957e-04, with an active set of 79 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 91 iterations, alpha=1.735e-04, previous alpha=1.733e-04, with an active set of 84 regressors.\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=7.379e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 69 iterations, alpha=2.596e-04, previous alpha=2.570e-04, with an active set of 66 regressors.\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 104 iterations, i.e. alpha=1.333e-04, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=8.200e-05, with an active set of 125 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 87 iterations, alpha=1.896e-04, previous alpha=1.890e-04, with an active set of 80 regressors.\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=1.343e-04, with an active set of 95 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.015e-04, with an active set of 110 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=8.972e-05, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=7.020e-05, with an active set of 128 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=6.995e-05, with an active set of 129 regressors, and the smallest cholesky pivot element being 4.829e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=6.995e-05, with an active set of 129 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=6.995e-05, with an active set of 129 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=6.994e-05, with an active set of 129 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 149 iterations, alpha=6.652e-05, previous alpha=6.520e-05, with an active set of 134 regressors.\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.022e-04, with an active set of 39 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.683e-04, with an active set of 60 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.054e-04, with an active set of 108 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.054e-04, with an active set of 108 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 120 iterations, alpha=1.033e-04, previous alpha=1.030e-04, with an active set of 109 regressors.\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=6.474e-04, with an active set of 27 regressors, and the smallest cholesky pivot element being 7.300e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.308e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 7.146e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=6.308e-04, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=4.405e-04, previous alpha=4.386e-04, with an active set of 45 regressors.\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.226e-04, with an active set of 106 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.101e-04, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.101e-04, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.101e-04, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 139 iterations, i.e. alpha=7.878e-05, with an active set of 127 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=5.505e-05, with an active set of 143 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=5.505e-05, with an active set of 143 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 178 iterations, i.e. alpha=4.313e-05, with an active set of 158 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 178 iterations, i.e. alpha=4.311e-05, with an active set of 158 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 178 iterations, i.e. alpha=4.277e-05, with an active set of 158 regressors, and the smallest cholesky pivot element being 3.650e-08\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 180 iterations, i.e. alpha=4.081e-05, with an active set of 160 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "c:\\python27\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 185 iterations, alpha=4.075e-05, previous alpha=3.946e-05, with an active set of 164 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the base model...\n",
      "Fitting the base model...\n",
      "Stacking base models...\n",
      "Param grid:\n",
      "{'alpha': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.8, 1.0, 3, 5, 7, 10.0]}\n",
      "Best Params:\n",
      "{'alpha': 0.5}\n",
      "Best CV Score:\n",
      "0.100948741275\n",
      "Best estimator:\n",
      "Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "('Creating submission: ', 'submission_0.100948741275_2017-05-06-15-19.csv')\n"
     ]
    }
   ],
   "source": [
    "ensem = ensemble(\n",
    "        n_folds=5,\n",
    "\tstacker=Ridge(),\n",
    "        base_models=base_models\n",
    "    )\n",
    "\n",
    "X_train,X_test,y_train = data_preprocess(train,test)\n",
    "y_pred, score = ensem.fit_predict(X_train,X_test,y_train)\n",
    "\n",
    "create_submission(np.expm1(y_pred),score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
